{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARSING DATASET\n",
      "\n",
      "Parsing  porcodioaugu.xml ...\n",
      "Parsing  porcodiolenz.xml ...\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATASET\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from load_data import parse_XML\n",
    "\n",
    "\n",
    "#Initialize data struct\n",
    "corpus_tg=\"/Users/chiarasemenzin/Desktop/MscProject/corpus/Tagged/\"\n",
    "corpus_ut=\"/Users/chiarasemenzin/Desktop/MscProject/corpus/Untagged/\"\n",
    "\n",
    "TAG_DATA=OrderedDict()\n",
    "\n",
    "\n",
    "# PARSE XML TAGGED\n",
    "print(\"PARSING DATASET\\n\")\n",
    "for root, dirs, files in os.walk(corpus_tg):\n",
    "    files = [ fi for fi in files if fi.endswith(\".xml\")]\n",
    "    for file in files:\n",
    "        print(\"Parsing \",file,\"...\")\n",
    "        phrase_list,lemmas=parse_XML.xml_iteration(corpus_tg+file)\n",
    "        TAG_DATA[file]=[phrase_list,lemmas]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing  1903AUGU-N.xml ...\n",
      "Parsing  1922AUGU.xml ...\n",
      "Parsing  1897LENZ-9.xml ...\n",
      "Parsing  1897LENZ-8.xml ...\n",
      "Parsing  1913GUEV-1.xml ...\n",
      "Parsing  1897LENZ-11.xml ...\n",
      "Parsing  1897LENZ-10.xml ...\n",
      "Parsing  1910AUGU-4.xml ...\n",
      "Parsing  1902AUGU.xml ...\n",
      "Parsing  1897LENZ-1.xml ...\n",
      "Parsing  1910AUGU-5.xml ...\n",
      "Parsing  1897LENZ-3.xml ...\n",
      "Parsing  1897LENZ-2.xml ...\n",
      "Parsing  1910AUGU-2.xml ...\n",
      "Parsing  1897LENZ-6.xml ...\n",
      "Parsing  1897LENZ-7.xml ...\n",
      "Parsing  1910AUGU-3.xml ...\n",
      "Parsing  1621VALD.xml ...\n",
      "Parsing  1910AUGU-1.xml ...\n",
      "Parsing  1897LENZ-5.xml ...\n",
      "Parsing  1897LENZ-4.xml ...\n",
      "Parsing  1765FEBR-2.xml ...\n",
      "Parsing  1765FEBR-3.xml ...\n",
      "Parsing  1765FEBR-1.xml ...\n",
      "Parsing  1903AUGU-1.xml ...\n",
      "Parsing  1930MOES.xml ...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# PARSE XML UNTAGGED\n",
    "\n",
    "UNTAG_DATA=OrderedDict()\n",
    "\n",
    "for root, dirs, files in os.walk(corpus_ut):\n",
    "    files = [ fi for fi in files if fi.endswith(\".xml\")]\n",
    "    for file in files:\n",
    "        print(\"Parsing \",file,\"...\")\n",
    "        try:\n",
    "            phrase_list=parse_XML.xml_iteration(corpus_ut+file,tagged=False)\n",
    "            UNTAG_DATA[file]=phrase_list\n",
    "        except:\n",
    "            continue\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens:  116548\n",
      "\n",
      "Total unique terms:  37027\n",
      "\n",
      "Top 10 types:  [('piam', 1965), ('feichi', 1728), ('kiñe', 1502), ('tañi', 1081), ('mapu', 1077), ('Veimeu', 734), ('piŋei', 693), ('tëfachi', 646), ('domo', 603), ('Dios', 573)]\n",
      "\n",
      "Sum top 10:  10602\n"
     ]
    }
   ],
   "source": [
    "## GET FREQUENCIES FUNCT DEF\n",
    "def get_freqs(pool):\n",
    "    freqs = {}\n",
    "    for word in pool:\n",
    "        if word not in freqs:\n",
    "            freqs[word] = 1\n",
    "        else:\n",
    "            freqs[word] += 1\n",
    "    sorted_fr=sorted(freqs.items(), key=lambda x:x[1])\n",
    "    sorted_fr.reverse()\n",
    "    return sorted_fr\n",
    "\n",
    "## POOL UNTAGGED DATA\n",
    "UNTAG_pool=[]\n",
    "for key, value in UNTAG_DATA.items():\n",
    "    for w in value:\n",
    "        UNTAG_pool.append(w)\n",
    "\n",
    "top_terms=get_freqs(UNTAG_pool)\n",
    "\n",
    "print(\"Total untagged tokens: \",(len(UNTAG_pool)))\n",
    "print(\"\\nTotal untagged unique terms: \",(len(top_terms)))\n",
    "print(\"\\nTop 10 types: \",top_terms[0:10])\n",
    "print(\"\\nSum frequencies top 10: \",sum([pair[1] for pair in top_terms[0:10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x114723710>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1/3 of the dataset is covered by the top 10 words\n",
    "# Zipfff\n",
    "\n",
    "import pandas \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "freqdf = pandas.DataFrame(top_terms[0:20], columns=['Word', 'Count']).set_index('Word')\n",
    "freqdf.plot.barh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tagged tokens:  3962\n",
      "Total types:  813\n"
     ]
    }
   ],
   "source": [
    "## POOL TAGGED DATA\n",
    "# import pool_tag_data()\n",
    "# pool_tag_data(TAGDATA)\n",
    "\n",
    "pool_tagged_sources=[]\n",
    "pool_tagged_targets=[]\n",
    "\n",
    "for key,value in TAG_DATA.items():\n",
    "    sources=value[0]\n",
    "    targets=value[1]\n",
    "    for i in sources:\n",
    "        for word in i:\n",
    "            pool_tagged_sources.append(word)\n",
    "            pool_tagged_targets.append(word)\n",
    "\n",
    "print(\"Total tagged tokens: \",(len((pool_tagged_sources))))\n",
    "print(\"Total types: \",(len(set(pool_tagged_sources))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unseen words:  36615\n",
      "Total seen words:  401\n"
     ]
    }
   ],
   "source": [
    "## GET UNSEEN WORDS\n",
    "\n",
    "words=set(UNTAG_pool)\n",
    "wordsu=set(pool_tagged_sources)\n",
    "\n",
    "print(\"Total unseen words: \",len(set(words)-set(wordsu)))\n",
    "print(\"Total seen words: \",len(set(wordsu)-set(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dummy_baseline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPELLING NORMALIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE THE LENGHT 2\n",
      "HERE THE LENGHT 2\n",
      "HERE THE LENGHT 2\n",
      "HERE THE LENGHT 2\n",
      "HERE THE LENGHT 2\n",
      "HERE THE LENGHT 2\n",
      "HERE THE LENGHT 2\n",
      "HERE THE LENGHT 2\n",
      "HERE THE LENGHT 2\n",
      "HERE THE LENGHT 2\n"
     ]
    }
   ],
   "source": [
    "# SPELL CHANGE \n",
    "\n",
    "from load_data import process_spelling\n",
    "\n",
    "changes=((\"ʎ\",\"ll\"),(\"t'\",\"tr\"),(\"ə\",\"ü\"),(\"f\",\"v\"),(\"k\",\"c\"))\n",
    "\n",
    "    \n",
    "UNTAG_DATA_norm=process_spelling.process_spelling(UNTAG_DATA,changes[0])\n",
    "TAG_DATA_norm=process_spelling.process_spelling(TAG_DATA,changes[0])\n",
    "\n",
    "UNTAG_DATA_norm=process_spelling.process_spelling(UNTAG_DATA_norm,changes[1])\n",
    "TAG_DATA_norm=process_spelling.process_spelling(TAG_DATA_norm,changes[1])\n",
    "\n",
    "UNTAG_DATA_norm=process_spelling.process_spelling(UNTAG_DATA_norm,changes[2])\n",
    "TAG_DATA_norm=process_spelling.process_spelling(TAG_DATA_norm,changes[2])\n",
    "\n",
    "UNTAG_DATA_norm=process_spelling.process_spelling(UNTAG_DATA_norm,changes[3])\n",
    "TAG_DATA_norm=process_spelling.process_spelling(TAG_DATA_norm,changes[3])\n",
    "\n",
    "UNTAG_DATA_norm=process_spelling.process_spelling(UNTAG_DATA_norm,changes[4])\n",
    "TAG_DATA_norm=process_spelling.process_spelling(TAG_DATA_norm,changes[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tagged tokens:  3962\n",
      "Total unique words:  770\n",
      "\n",
      "Total untagged tokens:  116548\n",
      "Total unique words:  36455\n"
     ]
    }
   ],
   "source": [
    "# POOL NORMALIZED DATA\n",
    "\n",
    "# TAG\n",
    "\n",
    "pool_tag_s_norm=[]\n",
    "pool_tag_t_norm=[]\n",
    "\n",
    "for key,value in TAG_DATA_norm.items():\n",
    "    sources=value[0]\n",
    "    targets=value[1]\n",
    "    for i in sources:\n",
    "        for word in i:\n",
    "            pool_tag_s_norm.append(word)\n",
    "            pool_tag_t_norm.append(word)\n",
    "\n",
    "print(\"Total tagged tokens: \",(len((pool_tag_s_norm))))\n",
    "print(\"Total unique words: \",(len(set(pool_tag_s_norm))))\n",
    "\n",
    "# UNTAG\n",
    "\n",
    "## POOL UNTAGGED DATA\n",
    "UNTAG_pool_norm=[]\n",
    "for key, value in UNTAG_DATA_norm.items():\n",
    "    for w in value:\n",
    "        UNTAG_pool_norm.append(w)\n",
    "\n",
    "print(\"\\nTotal untagged tokens: \",len(UNTAG_pool_norm))\n",
    "print(\"Total unique words: \",len(set(UNTAG_pool_norm)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unseen words:  36036\n",
      "\n",
      "Total seen words:  351\n"
     ]
    }
   ],
   "source": [
    "words=set(UNTAG_pool_norm)\n",
    "wordsu=set(pool_tag_s_norm)\n",
    "\n",
    "print(\"Total unseen words: \",len(set(words)-set(wordsu)))\n",
    "print(\"\\nTotal seen words: \",len(set(wordsu)-set(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less unique terms, less unseen words.\n",
    "  But also less seen words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FORMAT FOR LEMATUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chiarasemenzin/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    flat_words, flat_lemmas, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    flat_words, flat_lemmas, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spell Checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    pool_tag_s_norm, pool_tag_t_norm, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    pool_tag_s_norm, pool_tag_t_norm, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write to file - no context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('lematus/data/languages/Mapu-' + str(k) + '-char-context-' + str(n) + '-semi/train-sources', \"w\") as s:\n",
    "    #with open('lematus/data/languages/Mapu-' + str(k) + '-char-context-' + str(n) + '-semi/train-targets', \"w\") as t:\n",
    "\n",
    "\n",
    "# TRAIN\n",
    "with open('corpus/Tagged/train-sources', \"w\") as s:\n",
    "    with open('corpus/Tagged/train-targets', \"w\") as t:\n",
    "        for word, lemma in zip(X_train,y_train):\n",
    "            word=word.lower()\n",
    "            lemma=lemma.lower()\n",
    "            word=\" \".join(word)\n",
    "            lemma=\" \".join(lemma)\n",
    "            s.write(\"{}\\n\".format(word))\n",
    "            t.write(\"{}\\n\".format(lemma))\n",
    "s.close()\n",
    "t.close()\n",
    "\n",
    "\n",
    "# DEV \n",
    "with open('corpus/Tagged/dev-sources', \"w\") as s:\n",
    "    with open('corpus/Tagged/dev-targets', \"w\") as t:\n",
    "        for word, lemma in zip(X_val,y_val):\n",
    "            word=word.lower()\n",
    "            lemma=lemma.lower()\n",
    "            word=\" \".join(word)\n",
    "            lemma=\" \".join(lemma)\n",
    "            s.write(\"{}\\n\".format(word))\n",
    "            t.write(\"{}\\n\".format(lemma))\n",
    "s.close()\n",
    "t.close()\n",
    "\n",
    "\n",
    "# TEST\n",
    "with open('corpus/Tagged/test-sources', \"w\") as s:\n",
    "    with open('corpus/Tagged/test-targets', \"w\") as t:\n",
    "        for word, lemma in zip(X_test,y_test):\n",
    "            word=word.lower()\n",
    "            lemma=lemma.lower()\n",
    "            word=\" \".join(word)\n",
    "            lemma=\" \".join(lemma)\n",
    "            s.write(\"{}\\n\".format(word))\n",
    "            t.write(\"{}\\n\".format(lemma))\n",
    "s.close()\n",
    "t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def format hard att\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(porcodiaz[\"porcodiolenz.xml\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
